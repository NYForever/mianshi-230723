
# 面试资料整理


## java基础
#### [java基础](./1-jichu.md)

## 锁
#### [锁相关](./2-lock.md)

## 线程
#### [线程相关](./3-thread.md)

## 动态代理
#### [动态代理](./4-proxy.md)

## JVM虚拟机
#### [JVM虚拟机](./5-jvm.md)

## 数据库
#### [Mysql](./6-mysql.md)

#### [redis](./7-redis.md)

## 开源框架
### Spring
- 1.spring AOP是怎么实现的，怎么使用
- 2.spring Bean的生命周期，循环依赖spring是怎么解决的
- 3.spring AOP怎么实现的，和AspectJ有什么区别
- 4.spring AOP发生在Bean实力化的哪个阶段
- 5.只用二级缓存可以解决循环依赖吗
> 
> 如果要使用二级缓存解决循环依赖，意味着所有Bean在实例化后就要完成AOP代理，这样违背了Spring设计的原则，
> Spring在设计之初就是通过AnnotationAwareAspectJAutoProxyCreator这个后置处理器来在Bean生命周期的最后一步来完成AOP代理，而不是在实例化后就立马进行AOP代理。


### Mybatis
- 1.看过源码吗？底层如何实现的？

## 中间件
### RocketMQ
- 1.事物消息的执行流程
- 2.如何实现顺序消费？一定要单消费者吗
- 3.rocketmq rebalahce的原理是什么
```
RocketMQ的rebalance机制与Kafka有些不同，下面是RocketMQ rebalance的工作原理：

1. 当一个消费者加入或退出消费者组时，或者有新的主题（topic）被创建时，RocketMQ会触发rebalance过程。
2. 每个Broker节点上都有一个名为RebalanceService的线程，该线程负责处理rebalance操作。
3. RebalanceService定期（默认每20秒）从NameServer获取当前消费者组的订阅关系和消费者列表。
4. RebalanceService根据订阅关系和消费者列表计算出每个消费者应该处理的消息队列（Message Queue）集合。
5. 计算出的消息队列集合会通过Push模式推送给消费者，在消费者端进行分配和处理。
6. 消费者收到消息队列集合后，停止之前处理的消息队列并开始处理新分配的消息队列。
7. 在rebalance过程中，消费者会向Broker发送更新消费进度的请求，确保消息消费的顺序和一致性。
8. 任何消费者在rebalance期间无法读取或写入数据，因此rebalance过程需要尽量快速完成。

RocketMQ的rebalance机制采用Push模式将重新分配的消息队列信息推送给消费者。这样可以更快地通知消费者进行分区分配，并在消费者端实现动态扩展和负载均衡。同时，RocketMQ的rebalance过程也会考虑消费者端的负载情况，尽量使每个消费者处理相对均衡的消息队列。

需要注意的是，RocketMQ的rebalance机制是由Broker节点上的RebalanceService线程进行协调和执行的，而不是像Kafka那样由中心化的协调器节点。这种设计使得RocketMQ具有更好的可扩展性和容错性，因为rebalance操作与具体的Broker节点紧密结合，并且在整个集群中进行分布式执行。
```

### kafka
- 1.kafka是怎么保证高可用的，讲讲他的设计架构，为什么读写都在主分区，有什么优缺点
```
Kafka通过其设计架构和一些关键特性来保证高可用性。下面是关于Kafka高可用性的简要概述及其设计原则：

1. 分布式架构：Kafka采用分布式架构，允许在多个服务器或节点上部署多个副本。这样可以实现数据的冗余备份，以提供故障容错和高可用性。

2. 主从复制：Kafka使用主从复制的机制，将每个主题（Topic）的分区（Partition）复制到多个副本中。其中一个副本被指定为领导者（Leader），负责处理读写请求，而其他副本作为追随者（Follower）进行备份。

3. Leader-Follower同步：Kafka的领导者副本负责处理所有的读写请求，并将写入操作复制到追随者副本。追随者副本按照领导者副本的进度来进行数据同步，确保数据的一致性。只有当所有的追随者副本都成功同步数据后，才认为写入操作完成。

4. 分区负载均衡：每个主题可以被划分成多个分区，而每个分区只能有一个领导者副本。Kafka会自动进行分区负载均衡，将不同分区的领导者副本分布在不同的节点上，以实现负载均衡和高吞吐量。

读写都在主分区的优点和缺点如下：

优点：
- 简化设计：在单一的领导者副本上进行读写操作可以简化系统设计和实现。
- 减少数据复制：只需将写入操作复制到追随者副本即可，减少了无效的数据复制操作。
- 低延迟：由于读写操作只需要在一个副本上完成，所以具有较低的读写延迟。

缺点：
- 单点故障：如果主分区发生故障，整个分区将不可用，可能会影响系统的可用性。
- 读取瓶颈：所有的读取请求都必须通过主分区处理，可能导致主分区成为瓶颈，限制系统的读取吞吐量。
- 数据不一致：当主分区写入成功但尚未同步到所有追随者副本时，出现故障将导致数据不一致的情况。

总体而言，Kafka选择将读写操作集中在主分区是为了简化设计和降低延迟，并且利用主从复制机制来提供高可用性。然而，这种设计也带来了一些潜在的问题，如单点故障和读取瓶颈。为了解决这些问题，Kafka提供了故障转移机制和负载均衡策略，以实现更高级别的可用性和性能。
```
- 2.kafka的零拷贝怎么实现
- 3.kafka的reBalance raft算法
- 4.如何实现顺序消费？一定要单消费者吗
- 5.kafka用过吗，用的不多啊，rebalance说一下，就说一下是什么原理就行
```
Kafka的rebalance是指将Kafka集群中的分区（partitions）重新分配给消费者组（consumer group）中的消费者。它的目的是确保每个消费者对应的分区数量尽可能相等，以实现负载均衡，并允许新加入或退出的消费者能够动态地获取适当数量的分区。

以下是Kafka rebalance的工作原理：

1. 当一个消费者加入或退出消费者组时，或者有新的主题（topic）被创建时，Kafka会触发rebalance过程。
2. 当rebalance开始时，一个特殊的协调器（coordinator）节点被选举出来负责管理rebalance过程。
3. 协调器会与消费者组中的每个消费者通信，了解它们当前持有的分区情况。
4. 协调器根据一定的算法，计算出每个消费者应该持有的分区数量。
5. 接下来，协调器会将分区重新分配给消费者，并更新消费者的分区分配信息。
6. 消费者接收到新的分区分配信息后，会停止之前处理的分区并开始处理新分配的分区。
7. 任何消费者在rebalance期间无法读取或写入数据，因此rebalance过程需要尽量快速完成。
8. 一旦分区分配完成，消费者可以恢复数据处理，并继续消费主题中的消息。

总结来说，Kafka rebalance通过协调器节点的计算和分配过程，确保消费者组内每个消费者持有的分区数量尽可能均衡，实现高效的负载均衡和动态扩展。
```

### es
- 1.es的查询写入过程，底层实现，为什么这么设计
- 2.es集群脑裂问题，怎么产生的，怎么解决

### apollo
- 1.apollo的整体流程
```
Apollo 是一个开源的分布式配置中心，用于实现配置的集中管理和动态更新。下面是 Apollo 的整体流程：

1. 配置发布：
   - 开发人员使用 Apollo 提供的 Web 界面或 API 将配置发布到 Apollo 服务器上。
   - 发布过程会将配置存储在 Apollo 数据库中，并生成对应的配置文件。

2. 配置获取：
   - 应用程序通过 Apollo 客户端连接 Apollo 服务器，从服务器获取所需的配置信息。
   - 客户端可以根据需要指定获取的配置项、命名空间等信息。

3. 配置监听与更新：
   - Apollo 客户端在获取配置文件后会缓存在本地，并在启动时初始化配置。
   - 客户端通过长轮询或 WebSocket 等方式与 Apollo 服务器保持通信，实时监测配置变更。
   - 当配置发生变化时，Apollo 服务器会主动推送通知给客户端，客户端接收到通知后进行相应的更新操作。

4. 配置合并与生效：
   - Apollo 客户端在接收到配置更新通知后，将新的配置与本地缓存的配置进行合并。
   - 合并后的配置会覆盖原有的配置，使得配置变更生效。
   - 客户端可以根据自身需要选择何时更新配置，以最小化对业务的影响。

通过以上流程，Apollo 实现了配置的集中管理和动态更新。开发人员可以方便地使用 Apollo 进行配置的发布、获取和更新，而应用程序能够实时获取最新的配置信息，提高了配置管理的效率和灵活性。
```

### nacos&zk的区别是什么
- 1.nacos和zk的区别是什么
- 2.什么是CAP，为什么不能同时满足，redis是AP还是CP，nacos呢？zk呢？

## 监控
- 1.监控的traceId是怎么传递的，怎么做到链路追踪的

## 算法
- 1.雪花算法
- 2.最长回文子串
- 3.LRU算法怎么实现
> 在 Java 中实现 LRU（Least Recently Used）算法，你可以使用 LinkedHashMap 类来简化操作。LinkedHashMap 是一个哈希表和双向链表的结合体，它提供了按照插入顺序或访问顺序迭代元素的功能。
> 
> 以下是一种基于 LinkedHashMap 的 LRU 算法实现示例：
> 
> ```java
> import java.util.LinkedHashMap;
> import java.util.Map;
> 
> public class LRUCache<K, V> extends LinkedHashMap<K, V> {
>     private int capacity;
> 
>     public LRUCache(int capacity) {
>         super(capacity, 0.75f, true);
>         this.capacity = capacity;
>     }
> 
>     @Override
>     protected boolean removeEldestEntry(Map.Entry<K, V> eldest) {
>         return size() > capacity;
>     }
>     
>     // 可以根据需要添加其他自定义方法
>     
>     public static void main(String[] args) {
>         LRUCache<Integer, String> cache = new LRUCache<>(3);
>         
>         cache.put(1, "One");
>         cache.put(2, "Two");
>         cache.put(3, "Three");
>         
>         System.out.println(cache); // 输出：{1=One, 2=Two, 3=Three}
>         
>         cache.get(2);
>         
>         cache.put(4, "Four");
>         
>         System.out.println(cache); // 输出：{2=Two, 3=Three, 4=Four}
>     }
> }
> ```
> 
> 在上述代码中，LRUCache 类继承了 LinkedHashMap 并重写了 removeEldestEntry 方法，该方法会在每次插入新元素时被调用，判断是否需要移除最旧的元素（即是否超出容量）。通过设置 LinkedHashMap 的构造函数中的 accessOrder 参数为 true，可以实现按照访问顺序进行排序。默认情况下，LinkedHashMap 按照插入顺序进行排序。
> 
> 可以根据具体需求，添加其他自定义方法来操作缓存数据。以上示例仅提供了基本的插入和访问操作的演示。

## Linux
- 1.cpu 100%怎么排查
- 2.linux大文件，怎么查看某一行的内容
> 要查看大文件中的某一行内容，可以使用以下方法：
> 
> 1. 使用`head`和`tail`命令组合：假设你要查看第1000行的内容，可以先使用`head`命令查看前1000行，然后再使用`tail`命令查看最后一行，即可得到第1000行的内容。
> 
> ```shell
> head -n 1000 filename | tail -n 1
> ```
> 
> 这个方法适用于较大的文件，因为它只需要读取文件的一部分内容。
> 
> 2. 使用`sed`命令：`sed`是一个流编辑器，在处理大文件时效率较高。通过使用`sed`命令的行定位功能，可以直接获取特定行号的内容。
> 
> ```shell
> sed -n '1000p' filename
> ```
> 
> 这个命令将输出文件中的第1000行内容。使用`-n`参数表示只输出匹配规则的行，`1000p`表示输出第1000行。
> 
> 3. 使用`awk`命令：类似于`sed`命令，`awk`也是一种强大的文本处理工具。可以使用`awk`命令来定位并输出指定行号的内容。
> 
> ```shell
> awk 'NR==1000' filename
> ```
> 
> 上述命令使用了`NR==1000`条件，表示只输出行号为1000的行。
> 
> 以上三种方法都能实现在大文件中查看指定行内容，根据个人喜好和需求选择适合的方法。
- 3.linux的常用命令有哪些

## 架构设计
- 1.高并发架构的设计思路
- 2.限流降级熔断的区别是什么

## 网络通讯
### grpc 

- 1.protobuf底层是什么样的
- 2.grpc为什么快
> 1.序列化方式：
> RPC服务序列化是针对二进制协议（0/1）来做序列化和反序列化，所以性能高。
> 而Http服务是基于文本的序列化和反序列化，需要读一行一行的文本（比如json格式的），进行序列化和反序列化，所以性能低。
> 
> 2.报文长度：
> RPC服务是自定义的传输协议，传输的报文都是干货。
> 而Http服务里面包括很多没用的报文内容，比如Http Header里面的accept，referer等等
> 
> 3.连接的复用：
> RPC服务是基于TCP/IP协议的，是长连接。
> 而Http服务大都是短连接，虽然Http1.1支持长连接，但是这个也是要取决于服务端是否支持长连接，不太可控。
  

### TCP
- 1.IP协议在哪一层
- 2.TCP中拥塞控制和流量控制的区别是什么
- 3.TCP和UDP的区别在哪
- 4.浏览器输入URL后，解析到页面渲染中间都发生了什么
- 5.两台服务器之间可以同时建立多条TCP链接吗，怎么实现的


## 思维拓展 & 问题实践
- 1.假如有10亿个手机号，怎么快速判断一个手机号是否在其中
- 2.如何设计一个hash函数
```
设计一��良好的哈希函数是一个复杂的任务，需要综合考虑多个因素。下面是一些常见的设计原则和技巧来设计一个好的哈希函数：

1. 均匀性：一个好的哈希函数应该尽可能地将输入数据均匀地分布到哈希值空间中，以减少冲突的概率。

2. 简单性：哈希函数应当简单且高效，以保证计算速度。

3. 一致性：对于相同的输入，哈希函数应始终返回相同的哈希值，确保数据的一致性。

4. 抗碰撞能力：哈希函数应尽量避免冲突，即不同的输入产生相同的哈希值的概率要低。

5. 扩展性：在哈希表等数据结构中，哈希函数应具有良好的扩展性，允许动态增加或删除元素而不引起大规模的哈希冲突。

6. 混淆性：哈希函数应具有较高的“混淆”能力，即使输入相差很小的情况下，其生成的哈希值也应该有较大的区别。

7. 随机性：在某些特定场景下，可以使用随机数来设计哈希函数，以增加抗碰撞能力。

8. 哈希函数的长度：哈希函数的返回值长度应根据实际需求确定。通常，较长的哈希值可以减少碰撞的概率，但会占用更多的存储空间和计算资源。

需要注意的是，设计一个完美的哈希函数往往是不可能的，因为它涉及到复杂的数学和算法问题。在实际应用中，可以根据具体场景和需求选择现有的哈希函数（如 MD5、SHA-1、CRC32等）或者根据以上原则自行设计。此外，还可以使用一些哈希函数生成器工具来辅助设计和评估哈希函数的性能和效果。
```